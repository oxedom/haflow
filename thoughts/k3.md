# K3s Migration Plan for Haflow

**Date**: 2026-01-24
**Git Commit**: 68bb2b7145ad1f9f8dbc97de9c42ddabbc9fb917
**Branch**: main
**Repository**: haflow

---

## Executive Summary

Haflow's backend architecture already implements a **provider abstraction pattern** (`SandboxProvider` interface) that makes migrating from Docker to k3s relatively straightforward. The migration involves creating a new `k3sProvider` that implements the same interface, then switching a single line of code in the mission engine.

---

## Current Architecture

### Provider Interface (`packages/backend/src/services/sandbox.ts`)

The existing abstraction defines 8 methods:

```typescript
export interface SandboxProvider {
  start(options: SandboxRunOptions): Promise<string>;     // Launch sandbox â†’ return ID
  getStatus(containerId: string): Promise<SandboxStatus>; // Query state
  getLogTail(containerId: string, bytes?: number): Promise<string>;
  stop(containerId: string): Promise<void>;
  remove(containerId: string): Promise<void>;
  isAvailable(): Promise<boolean>;
  cleanupOrphaned(): Promise<void>;
  startClaudeStreaming?(options: ClaudeSandboxOptions): AsyncGenerator<StreamEvent>;
}
```

### Current Docker Implementation (`packages/backend/src/services/docker.ts`)

| Method | Docker Implementation |
|--------|----------------------|
| `start()` | `docker run -d` with labels, volume mounts, user mapping |
| `getStatus()` | `docker inspect --format='...'` |
| `getLogTail()` | `docker logs --tail 100` |
| `stop()` | `docker stop` |
| `remove()` | `docker rm -f` |
| `isAvailable()` | `docker version` |
| `cleanupOrphaned()` | `docker ps -aq --filter=label=haflow.mission_id` |
| `startClaudeStreaming()` | `docker sandbox run claude` (streaming) |

### Key Design Patterns

1. **Label-based tracking**: All containers tagged with `haflow.mission_id`, `haflow.run_id`, `haflow.step_id`
2. **Volume mount**: Artifacts at `${artifactsPath}:/mission/artifacts`
3. **Polling-based monitoring**: 1-second interval checking container status
4. **Detached execution**: Containers NOT using `--rm` to allow exit code inspection

---

## K3s Migration Strategy

### Phase 1: Infrastructure Setup

#### 1.1 K3s Installation

```bash
# Single-node installation (dev/staging)
curl -sfL https://get.k3s.io | sh -

# Verify installation
sudo k3s kubectl get nodes

# Copy kubeconfig for non-root access
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config
export KUBECONFIG=~/.kube/config
```

#### 1.2 Storage Configuration

K3s includes `local-path-provisioner` by default. For Haflow's artifact volume pattern:

```yaml
# Option A: PersistentVolumeClaim per mission (recommended)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: haflow-mission-{missionId}
  labels:
    haflow.mission_id: "{missionId}"
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 1Gi
```

```yaml
# Option B: hostPath volume (simpler, matches current Docker pattern)
volumes:
  - name: artifacts
    hostPath:
      path: /home/user/.haflow/missions/{missionId}/artifacts
      type: Directory
```

---

### Phase 2: Kubernetes Resource Definitions

#### 2.1 Job Manifest Template

Each agent step creates a Kubernetes Job:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: haflow-{runId}
  namespace: haflow
  labels:
    haflow.mission_id: "{missionId}"
    haflow.run_id: "{runId}"
    haflow.step_id: "{stepId}"
spec:
  ttlSecondsAfterFinished: 300  # Auto-cleanup after 5 minutes
  backoffLimit: 0                # No retries
  template:
    metadata:
      labels:
        haflow.run_id: "{runId}"
    spec:
      restartPolicy: Never
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
      containers:
        - name: agent
          image: node:20-slim
          workingDir: /mission
          command: ["sh", "-c"]
          args: ["{command}"]
          volumeMounts:
            - name: artifacts
              mountPath: /mission/artifacts
      volumes:
        - name: artifacts
          hostPath:
            path: "{artifactsPath}"
            type: Directory
```

#### 2.2 Namespace Setup

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: haflow
  labels:
    name: haflow
```

---

### Phase 3: K3s Provider Implementation

Create `packages/backend/src/services/k3s.ts`:

```typescript
import { KubeConfig, BatchV1Api, CoreV1Api, V1Job } from '@kubernetes/client-node';
import type { SandboxProvider, SandboxRunOptions, SandboxStatus } from './sandbox.js';

const NAMESPACE = 'haflow';
const LABEL_PREFIX = 'haflow';

// Initialize Kubernetes client
const kc = new KubeConfig();
kc.loadFromDefault();  // Uses KUBECONFIG or in-cluster config
const batchApi = kc.makeApiClient(BatchV1Api);
const coreApi = kc.makeApiClient(CoreV1Api);

export const k3sProvider: SandboxProvider = {
  async start(options: SandboxRunOptions): Promise<string> {
    const { missionId, runId, stepId, image, command, artifactsPath, workingDir = '/mission' } = options;

    const jobName = `haflow-${runId}`;
    const job: V1Job = {
      metadata: {
        name: jobName,
        namespace: NAMESPACE,
        labels: {
          [`${LABEL_PREFIX}.mission_id`]: missionId,
          [`${LABEL_PREFIX}.run_id`]: runId,
          [`${LABEL_PREFIX}.step_id`]: stepId,
        },
      },
      spec: {
        ttlSecondsAfterFinished: 300,
        backoffLimit: 0,
        template: {
          metadata: { labels: { [`${LABEL_PREFIX}.run_id`]: runId } },
          spec: {
            restartPolicy: 'Never',
            securityContext: { runAsUser: 1000, runAsGroup: 1000 },
            containers: [{
              name: 'agent',
              image: image || 'node:20-slim',
              workingDir,
              command: command.slice(0, 2),  // ['sh', '-c']
              args: command.slice(2),         // [script]
              volumeMounts: [{ name: 'artifacts', mountPath: `${workingDir}/artifacts` }],
            }],
            volumes: [{
              name: 'artifacts',
              hostPath: { path: artifactsPath, type: 'Directory' },
            }],
          },
        },
      },
    };

    await batchApi.createNamespacedJob(NAMESPACE, job);
    return jobName;  // Return job name as "containerId"
  },

  async getStatus(jobName: string): Promise<SandboxStatus> {
    try {
      const { body } = await batchApi.readNamespacedJobStatus(jobName, NAMESPACE);

      if (body.status?.succeeded) {
        return { state: 'exited', exitCode: 0, finishedAt: body.status.completionTime?.toISOString() };
      }
      if (body.status?.failed) {
        return { state: 'exited', exitCode: 1, finishedAt: body.status.conditions?.[0]?.lastTransitionTime?.toISOString() };
      }
      if (body.status?.active) {
        return { state: 'running', startedAt: body.status.startTime?.toISOString() };
      }
      return { state: 'unknown' };
    } catch {
      return { state: 'unknown' };
    }
  },

  async getLogTail(jobName: string, bytes = 2000): Promise<string> {
    try {
      // Find pod by job label
      const { body } = await coreApi.listNamespacedPod(NAMESPACE, undefined, undefined, undefined, undefined, `job-name=${jobName}`);
      const podName = body.items[0]?.metadata?.name;
      if (!podName) return '';

      const { body: logs } = await coreApi.readNamespacedPodLog(podName, NAMESPACE, 'agent', undefined, undefined, undefined, undefined, undefined, undefined, 100);
      return (logs || '').slice(-bytes);
    } catch {
      return '';
    }
  },

  async stop(jobName: string): Promise<void> {
    try {
      // Delete job with propagation to stop pod
      await batchApi.deleteNamespacedJob(jobName, NAMESPACE, undefined, undefined, undefined, undefined, 'Background');
    } catch { /* ignore */ }
  },

  async remove(jobName: string): Promise<void> {
    try {
      await batchApi.deleteNamespacedJob(jobName, NAMESPACE, undefined, undefined, undefined, undefined, 'Foreground');
    } catch { /* ignore */ }
  },

  async isAvailable(): Promise<boolean> {
    try {
      await coreApi.listNamespace();
      return true;
    } catch {
      return false;
    }
  },

  async cleanupOrphaned(): Promise<void> {
    try {
      // Delete all jobs with haflow labels
      await batchApi.deleteCollectionNamespacedJob(
        NAMESPACE, undefined, undefined, undefined, undefined, undefined,
        `${LABEL_PREFIX}.mission_id`
      );
    } catch { /* ignore */ }
  },

  // Claude streaming not implemented for k3s
  // Falls back to mock agent in mission-engine.ts:95-100
};
```

---

### Phase 4: Integration Changes

#### 4.1 Provider Selection (`packages/backend/src/services/mission-engine.ts:9`)

```typescript
// Option A: Environment-based selection
import { dockerProvider } from './docker.js';
import { k3sProvider } from './k3s.js';

const provider: SandboxProvider = process.env.HAFLOW_PROVIDER === 'k3s'
  ? k3sProvider
  : dockerProvider;
```

```typescript
// Option B: Config-based selection
import { config } from '../config.js';
const provider = config.sandboxProvider === 'k3s' ? k3sProvider : dockerProvider;
```

#### 4.2 Package Dependencies

```bash
pnpm --filter @haflow/backend add @kubernetes/client-node
```

#### 4.3 Configuration (`packages/backend/src/config.ts`)

```typescript
export const config = {
  // ... existing config
  sandboxProvider: process.env.HAFLOW_PROVIDER || 'docker',  // 'docker' | 'k3s'
  k3s: {
    namespace: process.env.K3S_NAMESPACE || 'haflow',
    kubeconfig: process.env.KUBECONFIG || undefined,
  },
};
```

---

### Phase 5: Migration Mapping

| Docker Concept | K3s Equivalent |
|----------------|----------------|
| Container | Pod (in Job) |
| `docker run -d` | `kubectl create job` |
| `docker inspect` | `kubectl get job -o json` |
| `docker logs` | `kubectl logs` |
| `docker stop` | `kubectl delete job` |
| `docker rm` | Job TTL auto-cleanup |
| `--label` | `metadata.labels` |
| `-v host:container` | `hostPath` or PVC volume |
| Container ID | Job name (`haflow-{runId}`) |

---

## Implementation Roadmap

### Step 1: Create K3s Provider
- [ ] Add `@kubernetes/client-node` dependency
- [ ] Create `packages/backend/src/services/k3s.ts`
- [ ] Implement all `SandboxProvider` methods
- [ ] Add unit tests with mocked k8s client

### Step 2: Add Provider Selection Logic
- [ ] Update `config.ts` with provider setting
- [ ] Update `mission-engine.ts` to select provider
- [ ] Add `HAFLOW_PROVIDER` environment variable

### Step 3: K3s Infrastructure
- [ ] Create namespace manifest (`haflow` namespace)
- [ ] Configure RBAC (ServiceAccount, Role, RoleBinding)
- [ ] Test with local k3s installation

### Step 4: Volume Strategy Decision
- [ ] Evaluate hostPath vs PVC for artifacts
- [ ] For multi-node: implement PVC-per-mission or shared storage
- [ ] Test artifact read/write in k3s pods

### Step 5: Testing
- [ ] Integration tests with real k3s
- [ ] Test polling/status detection
- [ ] Test log retrieval
- [ ] Test cleanup operations

### Step 6: Claude Streaming (Optional)
- [ ] Evaluate `kubectl exec` streaming approach
- [ ] Or: Run Claude agent as privileged pod with Docker socket
- [ ] Or: Defer to mock agent fallback

---

## Key Considerations

### 1. Claude Sandbox Streaming
The `startClaudeStreaming` method uses `docker sandbox run claude` which is Docker-specific. Options:
- **Option A**: Leave undefined (mission-engine falls back to mock agent at lines 95-100)
- **Option B**: Run Claude in a privileged pod with Docker socket access
- **Option C**: Implement `kubectl exec` streaming into Claude container

### 2. Volume Strategy
- **Single-node**: `hostPath` volume (matches current Docker pattern)
- **Multi-node**: PersistentVolumeClaim with Longhorn or NFS

### 3. State Mapping

| SandboxStatus | K8s Job Status |
|---------------|----------------|
| `running` | `status.active > 0` |
| `exited` (success) | `status.succeeded > 0` |
| `exited` (failure) | `status.failed > 0` |
| `unknown` | Error or pending |

### 4. Resource Limits (Recommended)
```yaml
resources:
  limits:
    cpu: "2"
    memory: "2Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"
```

---

## Files to Create/Modify

| File | Action | Purpose |
|------|--------|---------|
| `packages/backend/src/services/k3s.ts` | Create | K3s provider implementation |
| `packages/backend/src/services/mission-engine.ts` | Modify line 9 | Provider selection |
| `packages/backend/src/config.ts` | Modify | Add k3s config options |
| `packages/backend/package.json` | Modify | Add `@kubernetes/client-node` |
| `k8s/namespace.yaml` | Create | Namespace manifest |
| `k8s/rbac.yaml` | Create | ServiceAccount + RBAC |
| `packages/backend/tests/unit/services/k3s.test.ts` | Create | Unit tests |

---

## Code References

- **Provider interface**: `packages/backend/src/services/sandbox.ts:38-80`
- **Docker implementation**: `packages/backend/src/services/docker.ts:287-296`
- **Provider assignment**: `packages/backend/src/services/mission-engine.ts:9`
- **Polling loop**: `packages/backend/src/services/mission-engine.ts:292-336`
- **Streaming fallback**: `packages/backend/src/services/mission-engine.ts:95-100`
- **Label prefix**: `packages/backend/src/services/docker.ts:8`

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Volume permission issues | High | Medium | Match UID/GID in securityContext |
| Log retrieval timing | Medium | Low | Add retry logic in getLogTail |
| Job cleanup race | Low | Low | Use ttlSecondsAfterFinished |
| Claude streaming unavailable | High | Medium | Accept mock agent fallback |
| Multi-node storage | Medium | High | Use Longhorn or defer to single-node |

---

## K3s Quick Reference

### What is K3s?
- Lightweight Kubernetes distribution (~100MB binary)
- Single binary containing: containerd, Flannel CNI, CoreDNS, Traefik ingress, local-path-provisioner
- Minimal dependencies: modern kernel + cgroup mounts
- Default storage: SQLite (etcd3, MySQL, Postgres also supported)

### Built-in Components
- **Container Runtime**: containerd
- **Networking**: Flannel (CNI)
- **DNS**: CoreDNS
- **Ingress**: Traefik
- **Load Balancing**: ServiceLB
- **Storage**: local-path-provisioner (default `local-path` StorageClass)

### Key Commands
```bash
# Install k3s
curl -sfL https://get.k3s.io | sh -

# Access kubectl
sudo k3s kubectl get nodes

# Kubeconfig location
/etc/rancher/k3s/k3s.yaml

# Add worker node
curl -sfL https://get.k3s.io | K3S_URL=https://server:6443 K3S_TOKEN=<token> sh -

# Token location
/var/lib/rancher/k3s/server/node-token
```

---

**End of Migration Plan**
